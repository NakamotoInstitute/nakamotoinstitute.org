---
title: Wet Code and Dry
authors:
  - nick-szabo
date: 2006-11-14
categories:
  - computer-science
doctype: essay
external: https://unenumerated.blogspot.com/2006/11/wet-code-and-dry.html
---

There's a strong distinction to be made between "wet code," interpreted by the brain, and "dry code," interpreted by computers. Human-read media is wet code whereas computer code and computer-readable files (to the extent a computer deals meaningfully with them) are "dry code." Law is wet code, interpreted by those on whom the law is imposed, and interpreted (often somewhat differently) by law enforcers, but most authoritatively (and even more differently) interpreted by judges. Human language is mostly wet code but to the extent computer programs crudely translate from one language to another, keyword-ad programs parse text to made an educated guess as to what ads a user will most likely click, and so on, human language text can also be dry code. Traditional contracts are wet code whereas [smart contracts](/library/formalizing-sercuring-relationships/) are mostly dry code. [Secure property titles](/library/secure-property-titles/) and the [domain name system](http://en.wikipedia.org/wiki/DNS) are mostly dry code.

Even "mostly dry code" often has surprisingly soggy portions. Smart contracts, for example, can use dialogs to communicate their nature to the user and to allow the user to at least input some parameters, make menu choices, and the like. These are [cognitive channels](http://www.truststc.org/pubs/132/Annarita_Giani_Berkeley.ppt) that translate between wet code and dry code. Similarly, the human-readable name in the domain name system is wet code and trademark law is wet code that applies to domain names. [Phishing](http://en.wikipedia.org/wiki/Phishing) is an attack against cognitive channels using ambiguous wet code, which is probably why dry code programmers have a hard time coming to grips with it.

The syntactic properties of wet and dry code are often very similar -- for example, both human and computer language can usually be parsed by [pushdown automata](http://en.wikipedia.org/wiki/Pushdown_automata) into [context-free grammars](http://en.wikipedia.org/wiki/Context-free_grammar). But their semantic content is often radically different, and the semantic content of wet code is often simply unintelligible to a computer, for a variety of often mysterious reasons. If we had good enough theories of human semantics we could program such theories into the computer and the computer would then understand like a brain after all. But we don't, in most areas, so we mostly can't program computers to emulate humans. I believe these mysteries are primarily due to the highly evolved nature of the brain contrasted to the recent and thus naive designs of computers, especially with respect to computers' typical lack of good sensors (they're mostly still far more sensory-deprived than [Helen Keller](http://en.wikipedia.org/wiki/Helen_Keller), who had a very informative sense of touch) and relative inability to learn from natural and social environments. Their ability to aggregate disparate kinds of information, for example through [conceptual metaphor](http://en.wikipedia.org/wiki/Conceptual_metaphor), is also relatively undeveloped.

As computers become "smarter" dry code is (very slowly) coming to do more that was formerly only done by wet code. Once it successfully emulates wet code it soon surpasses it in many respects; for example by now dry code can do simple arithmetic billions of times faster than the typical human. As the idea of smart contracts suggests, there are many fruitful analogies to be made from wet code to dry code, but we must keep in mind the radically different semantics, the strengths and weakness of each, and the need for each to interact with the other to solve real problems.

I don't think there is a "magic bullet" theory of artificial intelligence that will uncover the semantic mysteries and give computers intelligence in one fell swoop. I don't think that computers will mysteriously "wake up" one day in some magic transition from [zombie](http://en.wikipedia.org/wiki/Philosophical_zombie) to [qualia.](http://en.wikipedia.org/wiki/Qualia) (I basically agree with [Daniel Dennett](http://en.wikipedia.org/wiki/Daniel_Dennett) in this respect). Instead, we will continue to chip away at formalizing human intelligence, a few "bits" at a time, and will never reach a "singularity" where all of a sudden we one day way wake up and realize computers have surpassed us. Instead, there will be numerous "micro-runaways" for particular narrow abilities that we learn how to teach computers to do, such as the runaway over the last century or so in the superiority of computers over humans in basic arithmetic. Computers and humans will continue to co-evolve with computers making the faster progress but falling far short of apocalyptic predictions of "[Singularity](http://en.wikipedia.org/wiki/Technological_singularity)," except to the extent that much of civilization is already a rolling singularity. For example people can't generally predict what's going to happen next in markets or which new startups will succeed in the long run.
